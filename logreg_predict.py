import sys
import pandas as pd
import numpy as np
import json


def load_weights(filepath='weights.json'):
    """Charge les poids et paramÃ¨tres depuis le fichier JSON"""
    try:
        # Ouvrir et lire le fichier JSON
        with open(filepath, 'r') as f:
            data = json.load(f)
    except FileNotFoundError:
        print(f"âŒ Erreur : Le fichier {filepath} n'existe pas")
        print("   Lance d'abord logreg_train.py pour crÃ©er les poids")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ Erreur lors du chargement : {e}")
        sys.exit(1)

    # Extraire les 4 Ã©lÃ©ments du dictionnaire
    feature_names = data['feature_names']

    # Convertir means et stds en numpy arrays pour les calculs
    means = np.array(data['means'])
    stds = np.array(data['stds'])

    # Laisser weights en listes pour l'instant (converti plus tard dans
    # predict)
    weights = data['weights']

    print(f"âœ… Poids chargÃ©s depuis : {filepath}")
    print(f"   Features : {len(feature_names)}")
    print(f"   Maisons  : {list(weights.keys())}")

    return feature_names, means, stds, weights


def load_data(filepath, feature_names):
    """Charge le test set et extrait les features"""
    # Charger le CSV
    try:
        df = pd.read_csv(filepath)
        print(f"âœ… Fichier chargÃ© : {filepath}")
        print(f"   {len(df)} Ã©tudiants Ã  prÃ©dire")
    except FileNotFoundError:
        print(f"âŒ Erreur : Le fichier {filepath} n'existe pas")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ Erreur lors du chargement : {e}")
        sys.exit(1)

    # VÃ©rifier que toutes les features existent
    for feat in feature_names:
        if feat not in df.columns:
            print(f"âŒ Erreur : Feature '{feat}' introuvable dans le test set")
            sys.exit(1)

    # Extraire les bonnes colonnes et convertir en numpy array
    X = df[feature_names].values

    print(f"âœ… Features extraites : {len(feature_names)}")
    print(f"   Shape de X : {X.shape}")

    return X


def handle_missing_values(X, means):
    """
    Remplace les NaN par les moyennes du train set

    Args:
        X: numpy array (m, n) - peut contenir des NaN
        means: numpy array (n,) - moyennes de chaque feature (du train)

    Returns:
        X_clean: numpy array (m, n) - sans NaN

    Note:
        Utilise les moyennes du TRAIN SET (pas du test set!)
        C'est crucial pour Ã©viter le data leakage
    """
    X_clean = X.copy()
    m, n = X.shape
    total_nans = 0

    # Pour chaque colonne
    for j in range(n):
        # Trouver les NaN dans cette colonne
        nan_mask = np.isnan(X_clean[:, j])
        nan_count = np.sum(nan_mask)

        if nan_count > 0:
            # Remplacer par la moyenne du train
            X_clean[nan_mask, j] = means[j]
            total_nans += nan_count

    print(f"âœ… Valeurs manquantes traitÃ©es : {total_nans} NaN remplacÃ©s")
    if total_nans > 0:
        print("   StratÃ©gie : moyennes du train set")

    return X_clean


def standardize(X, means, stds):
    """
    Standardise avec les paramÃ¨tres du train set

    Args:
        X: numpy array (m, n)
        means: numpy array (n,) - moyennes du train
        stds: numpy array (n,) - Ã©carts-types du train

    Returns:
        X_norm: numpy array (m, n) - donnÃ©es standardisÃ©es

    Note:
        Utilise les MÃŠMES moyennes et Ã©carts-types que le train!
        Formule : X_norm = (X - means) / stds
    """
    X_norm = X.copy()

    # Pour chaque colonne
    for j in range(X.shape[1]):
        if stds[j] > 0:
            X_norm[:, j] = (X[:, j] - means[j]) / stds[j]
        else:
            # Si std = 0 dans le train, juste centrer
            X_norm[:, j] = X[:, j] - means[j]

    print("âœ… Standardisation appliquÃ©e (paramÃ¨tres du train)")
    print(f"   X_norm : min={X_norm.min():.2f}, max={X_norm.max():.2f}")

    return X_norm


def add_intercept(X):
    """
    Ajoute une colonne de 1 au dÃ©but

    Args:
        X: numpy array (m, n)

    Returns:
        X_with_intercept: numpy array (m, n+1)
    """
    m = X.shape[0]
    ones = np.ones(m)
    X_with_intercept = np.c_[ones, X]

    print("âœ… Colonne d'intercept ajoutÃ©e")
    print(f"   Shape : {X.shape} â†’ {X_with_intercept.shape}")

    return X_with_intercept


def sigmoid(z):
    """Fonction sigmoÃ¯de pour la rÃ©gression logistique"""
    # Clipper z pour Ã©viter l'overflow (e^1000 = trop grand)
    z = np.clip(z, -500, 500)
    return 1 / (1 + np.exp(-z))


def predict(X, weights):
    """
    PrÃ©dit les maisons pour les donnÃ©es X

    Args:
        X: numpy array (m, n) - donnÃ©es prÃ©parÃ©es (avec intercept)
        weights: dict - poids de chaque maison

    Returns:
        predictions: list - maison prÃ©dite pour chaque Ã©tudiant
    """
    # Convertir les poids en numpy array pour le calcul
    house_names = list(weights.keys())
    weight_matrix = np.array([weights[house] for house in house_names])
    # shape (num_houses, n)

    # Calculer les scores pour chaque maison
    scores = X @ weight_matrix.T  # shape (m, num_houses)

    # Appliquer la sigmoÃ¯de pour obtenir des probabilitÃ©s
    probabilities = sigmoid(scores)

    # PrÃ©dire la maison avec la probabilitÃ© la plus Ã©levÃ©e
    predicted_indices = np.argmax(probabilities, axis=1)
    predictions = [house_names[idx] for idx in predicted_indices]

    return predictions


def save_predictions(predictions, output_filepath='houses.csv'):
    """
    Sauvegarde les prÃ©dictions dans un fichier CSV

    Format requis par le sujet :
        Index,Hogwarts House
        0,Gryffindor
        1,Hufflepuff
        ...
    """
    # CrÃ©er un DataFrame avec le format exact demandÃ©
    df_output = pd.DataFrame({
        'Index': range(len(predictions)),
        'Hogwarts House': predictions
    })

    # Sauvegarder avec index=False pour Ã©viter une colonne index supplÃ©mentaire
    df_output.to_csv(output_filepath, index=False)

    print(f"âœ… PrÃ©dictions sauvegardÃ©es dans : {output_filepath}")
    print("   Format : Index,Hogwarts House")
    print(f"   {len(predictions)} prÃ©dictions")


def main():
    """Point d'entrÃ©e principal"""
    # RÃ©cupÃ©rer les fichiers
    filepath_test = sys.argv[1] if len(sys.argv) > 1 \
        else 'data/dataset_test.csv'
    filepath_weights = sys.argv[2] if len(sys.argv) > 2 \
        else 'weights.json'

    print("=" * 60)
    print("âš¡ POUDLARD - PRÃ‰DICTION DES MAISONS âš¡")
    print("=" * 60)
    print()

    # Ã‰tape 1 : Charger les poids
    print("ğŸ“‚ Ã‰TAPE 1 : Chargement des poids")
    print("-" * 60)
    feature_names, means, stds, weights = load_weights(filepath_weights)
    print()

    # Ã‰tape 2 : Charger le test set
    print("ğŸ“‚ Ã‰TAPE 2 : Chargement du test set")
    print("-" * 60)
    X = load_data(filepath_test, feature_names)
    print()

    # Ã‰tape 3 : PrÃ©parer X (mÃªme pipeline que le train)
    print("ğŸ”§ Ã‰TAPE 3 : PrÃ©paration des donnÃ©es")
    print("-" * 60)

    # 3a. GÃ©rer les NaN avec les moyennes du train
    X = handle_missing_values(X, means)

    # 3b. Standardiser avec les paramÃ¨tres du train
    X = standardize(X, means, stds)

    # 3c. Ajouter intercept
    X = add_intercept(X)
    print()

    # Ã‰tape 4 : PrÃ©dire les maisons
    print("ğŸ¯ Ã‰TAPE 4 : PrÃ©diction des maisons")
    print("-" * 60)
    predictions = predict(X, weights)
    print(f"âœ… PrÃ©dictions gÃ©nÃ©rÃ©es : {len(predictions)} Ã©tudiants")
    print(f"   Exemples : {predictions[:5]}")
    print()

    # Ã‰tape 5 : Sauvegarder les prÃ©dictions
    print("ğŸ’¾ Ã‰TAPE 5 : Sauvegarde des prÃ©dictions")
    print("-" * 60)
    save_predictions(predictions, 'houses.csv')
    print()
    print("=" * 60)
    print("âš¡ PRÃ‰DICTION TERMINÃ‰E !")
    print("=" * 60)


if __name__ == '__main__':
    main()
