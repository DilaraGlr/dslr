import sys
import pandas as pd
import numpy as np
import json


def load_data(filepath, selected_features=None):
    """Charge les donnÃ©es depuis le CSV et extrait X et y"""
    # 1. Charger le CSV
    try:
        df = pd.read_csv(filepath)
        print(f"âœ… Fichier chargÃ© : {filepath}")
        print(f"   {len(df)} Ã©tudiants trouvÃ©s")
    except FileNotFoundError:
        print(f"âŒ Erreur : Le fichier {filepath} n'existe pas")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ Erreur lors du chargement : {e}")
        sys.exit(1)

    # 2. Extraire y (labels - les maisons)
    if 'Hogwarts House' not in df.columns:
        print("âŒ Erreur : Colonne 'Hogwarts House' introuvable")
        sys.exit(1)

    y = df['Hogwarts House'].copy()

    # VÃ©rifier qu'il n'y a pas de NaN dans y
    nan_count = y.isna().sum()
    if nan_count > 0:
        print(f"âš ï¸  Attention : {nan_count} valeurs manquantes dans "
              f"'Hogwarts House'")
        # On retire les lignes avec NaN dans y
        valid_indices = y.notna()
        y = y[valid_indices]
        df = df[valid_indices]
        print(f"   â†’ {len(y)} Ã©tudiants conservÃ©s aprÃ¨s nettoyage")

    # 3. Extraire X (features - les cours)
    # Colonnes Ã  exclure (non-numÃ©riques ou non-pertinentes)
    non_feature_columns = ['Index', 'Hogwarts House', 'First Name',
                           'Last Name', 'Birthday', 'Best Hand']

    # DÃ©terminer les features Ã  utiliser
    if selected_features is not None:
        # Utiliser les features sÃ©lectionnÃ©es (d'aprÃ¨s l'analyse EDA)
        feature_columns = selected_features
        print("ğŸ¯ Utilisation des features sÃ©lectionnÃ©es (analyse EDA)")
    else:
        # Utiliser toutes les colonnes numÃ©riques
        feature_columns = []
        for col in df.columns:
            if col not in non_feature_columns:
                # VÃ©rifier que c'est bien numÃ©rique
                if df[col].dtype in ['float64', 'int64']:
                    feature_columns.append(col)
        print("ğŸ“Š Utilisation de toutes les features numÃ©riques")

    if len(feature_columns) == 0:
        print("âŒ Erreur : Aucune feature numÃ©rique trouvÃ©e")
        sys.exit(1)

    # VÃ©rifier que toutes les features demandÃ©es existent
    for col in feature_columns:
        if col not in df.columns:
            print(f"âŒ Erreur : Feature '{col}' introuvable dans le dataset")
            sys.exit(1)

    # Convertir X en numpy array pour les calculs matriciels
    X = df[feature_columns].values

    print(f"âœ… Features sÃ©lectionnÃ©es : {len(feature_columns)} cours")
    for i, feat in enumerate(feature_columns, 1):
        print(f"   {i}. {feat}")
    print(f"\nâœ… Labels : {y.nunique()} maisons")
    print(f"   {sorted(y.unique())}")
    print(f"\nğŸ“ Shape de X : {X.shape} (samples, features)")

    return X, y, feature_columns


def handle_missing_values(X):
    """Remplace les valeurs manquantes (NaN) par la moyenne de leur colonne"""
    # CrÃ©er une copie pour ne pas modifier l'original
    X_clean = X.copy()

    m, n = X.shape  # m = nombre d'Ã©tudiants, n = nombre de features
    total_nans = 0

    # Pour chaque colonne (chaque feature)
    for j in range(n):
        # Extraire la colonne j
        column = X[:, j]

        # Calculer la moyenne
        total = 0.0
        count = 0

        # Parcourir toutes les valeurs de la colonne
        for i in range(m):
            val = column[i]
            # VÃ©rifier si c'est un NaN
            if not np.isnan(val):
                total += val
                count += 1

        # Si toute la colonne est NaN
        if count == 0:
            mean = 0.0
            print(f"âš ï¸  Colonne {j} entiÃ¨rement NaN â†’ moyenne = 0")
        else:
            mean = total / count

        # Compter et remplacer les NaN dans cette colonne
        # Utiliser un masque boolÃ©en pour identifier les NaN
        nan_mask = np.isnan(X_clean[:, j])
        nan_count = np.sum(nan_mask)

        if nan_count > 0:
            # Remplacer tous les NaN de cette colonne par la moyenne
            X_clean[nan_mask, j] = mean
            total_nans += nan_count

    print(f"âœ… Valeurs manquantes traitÃ©es : {total_nans} NaN remplacÃ©s")
    if total_nans > 0:
        print("   StratÃ©gie : remplacement par la moyenne de chaque colonne")

    return X_clean


def standardize(X):
    """Mettre toutes les features Ã  la mÃªme Ã©chelle (z-score normalization)
    Description:
        Pour chaque colonne :
        1. Calculer manuellement la moyenne Î¼
        2. Calculer manuellement l'Ã©cart-type Ïƒ
           Ïƒ = âˆš(Î£(x - Î¼)Â² / m)
        3. Normaliser : x_norm = (x - Î¼) / Ïƒ
    """
    m, n = X.shape
    X_norm = X.copy()

    # Tableaux pour stocker les moyennes et Ã©carts-types
    means = np.zeros(n)
    stds = np.zeros(n)

    # Pour chaque colonne (chaque feature)
    for j in range(n):
        column = X[:, j]

        # 1. Calculer la moyenne
        total = 0.0
        for i in range(m):
            total += column[i]
        mean = total / m
        means[j] = mean

        # 2. Calculer l'Ã©cart-type
        # std = âˆš(Î£(x - mean)Â² / m)
        sum_squared_diff = 0.0
        for i in range(m):
            diff = column[i] - mean
            sum_squared_diff += diff * diff

        variance = sum_squared_diff / m
        std = variance ** 0.5  # Racine carrÃ©e
        stds[j] = std

        # 3. Normaliser la colonne
        # Si std = 0 (colonne constante), on ne divise pas pour Ã©viter NaN
        if std > 0:
            X_norm[:, j] = (column - mean) / std
        else:
            # Colonne constante â†’ on centre juste (- mean)
            X_norm[:, j] = column - mean
            print(f"âš ï¸  Colonne {j} a un Ã©cart-type nul â†’ centrÃ©e seulement")

    print("âœ… Standardisation terminÃ©e (z-score)")
    print(f"   Moyennes : min={means.min():.2f}, max={means.max():.2f}")
    print(f"   Ã‰carts-types : min={stds.min():.2f}, max={stds.max():.2f}")
    print(f"   X_norm : min={X_norm.min():.2f}, max={X_norm.max():.2f}")

    return X_norm, means, stds


def add_intercept(X):
    """
    Ajoute une colonne de 1 au dÃ©but de la matrice X (intercept/biais)

    Description:
        Ajoute une colonne de 1 tout Ã  gauche de X.
        Cette colonne permet de calculer le terme d'intercept (Î¸â‚€)
        automatiquement dans le produit matriciel z = X @ Î¸

        Exemple:
            X = [[x1, x2],     â†’    X_intercept = [[1, x1, x2],
                 [x3, x4]]                          [1, x3, x4]]

        Shape : (m, n) â†’ (m, n+1)
    """
    m = X.shape[0]  # Nombre d'Ã©chantillons

    # CrÃ©er une colonne de 1 de taille (m,)
    ones = np.ones(m)

    # ConcatÃ©ner horizontalement : [colonne de 1] + [X]
    # np.c_ permet de concatÃ©ner des colonnes
    X_with_intercept = np.c_[ones, X]

    print("âœ… Colonne d'intercept ajoutÃ©e")
    print(f"   Shape avant : {X.shape}")
    print(f"   Shape aprÃ¨s : {X_with_intercept.shape}")

    return X_with_intercept


def sigmoid(z):
    """
    Transforme un score brut en probabilitÃ© (entre 0 et 1)"""
    # Limiter z pour Ã©viter l'overflow numÃ©rique
    z = np.clip(z, -500, 500)

    # Appliquer la formule sigmoid
    return 1 / (1 + np.exp(-z))


def cost_function(h, y):
    """
    Calcule la log-loss (Binary Cross-Entropy)

    Args:
        h: numpy array (m,) - probabilitÃ©s prÃ©dites par sigmoid (entre 0 et 1)
        y: numpy array (m,) - vraies valeurs binaires (0 ou 1)

    Returns:
        float - valeur de la loss (plus elle est basse, mieux c'est)

    Formule:
        J = -1/m * Î£(y * log(h) + (1 - y) * log(1 - h))

        - y * log(h)       : pÃ©nalise si la rÃ©ponse est 1 et h proche de 0
        - (1-y) * log(1-h) : pÃ©nalise si la rÃ©ponse est 0 et h proche de 1
    """
    m = len(y)

    # Clipper h pour Ã©viter log(0) qui donne -inf et crashe tout
    h = np.clip(h, 1e-15, 1 - 1e-15)

    # Calculer la log-loss
    # Partie 1 : cas oÃ¹ la vraie rÃ©ponse est 1 â†’ y * log(h)
    # Partie 2 : cas oÃ¹ la vraie rÃ©ponse est 0 â†’ (1 - y) * log(1 - h)
    loss = -1 / m * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))

    return loss


def compute_gradient(X, h, y):
    """
    Calcule le gradient de la loss par rapport aux poids theta

    Args:
        X: numpy array (m, n+1) - donnÃ©es avec intercept
        h: numpy array (m,)    - probabilitÃ©s prÃ©dites par sigmoid
        y: numpy array (m,)    - vraies valeurs binaires (0 ou 1)

    Returns:
        grad: numpy array (n+1,) - gradient pour chaque poids

    Formule vectorisÃ©e:
        grad = (1/m) * X.T @ (h - y)

        - (h - y)   : erreur entre prÃ©diction et vÃ©ritÃ© (m,)
        - X.T       : X transposÃ©, shape (n+1, m)
        - X.T @ (h-y) : produit matriciel â†’ un gradient par poids (n+1,)
        - 1/m       : moyenne sur tous les Ã©tudiants
    """
    m = len(y)

    grad = (1 / m) * X.T @ (h - y)

    return grad


def train_one_vs_all(X, y, learning_rate=0.1, epochs=1000):
    """
    EntraÃ®ne 4 classifieurs binaires (un par maison)

    Args:
        X: numpy array (m, n+1) - donnÃ©es avec intercept
        y: pandas Series        - labels (noms des maisons)
        learning_rate: float    - taille du pas du gradient descent
        epochs: int             - nombre d'itÃ©rations d'entraÃ®nement

    Returns:
        weights: dict - {'Gryffindor': theta, 'Slytherin': theta, ...}
                        un tableau de poids pour chaque maison

    Description:
        Pour chaque maison :
        1. Convertir y en vecteur binaire (1 = cette maison, 0 = autre)
        2. Initialiser theta Ã  zÃ©ro
        3. RÃ©pÃ©ter `epochs` fois :
           a. h = sigmoid(X @ theta)        â†’ probabilitÃ©s
           b. loss = compute_loss(h, y_bin) â†’ mesure l'erreur
           c. grad = compute_gradient(...)  â†’ direction de correction
           d. theta = theta - lr * grad     â†’ ajuster les poids
    """
    # RÃ©cupÃ©rer les 4 maisons uniques
    houses = sorted(y.unique())

    # Dictionnaire pour stocker les poids de chaque maison
    weights = {}

    print(f"EntraÃ®nement : {len(houses)} maisons, "
          f"{epochs} epochs, lr={learning_rate}")
    print()

    # Pour chaque maison
    for house in houses:
        print(f"  ğŸ° {house}...")

        # 1. Convertir y en vecteur binaire
        # 1 = c'est cette maison, 0 = c'est une autre maison
        y_binary = (y == house).astype(int).values

        # 2. Initialiser theta Ã  zÃ©ro (un poids par feature + biais)
        theta = np.zeros(X.shape[1])

        # 3. Boucle de gradient descent
        for epoch in range(epochs):
            # a. Calculer les probabilitÃ©s prÃ©dites
            h = sigmoid(X @ theta)

            # b. Mesurer l'erreur (loss)
            loss = cost_function(h, y_binary)

            # c. Calculer le gradient
            grad = compute_gradient(X, h, y_binary)

            # d. Ajuster les poids
            theta = theta - learning_rate * grad

            # Afficher la progression toutes les 200 epochs
            if (epoch + 1) % 200 == 0:
                print(f"     epoch {epoch + 1}/{epochs} â†’ loss = {loss:.4f}")

        # Sauvegarder les poids de cette maison
        weights[house] = theta

    print()
    print(f"âœ… EntraÃ®nement terminÃ© ! {len(weights)} modÃ¨les entraÃ®nÃ©s")

    return weights


def save_weights(weights, means, stds, feature_names, filepath='weights.json'):
    """
    Sauvegarde les poids et paramÃ¨tres de normalisation dans un fichier JSON

    Args:
        weights: dict        - {'Gryffindor': theta, ...} (numpy arrays)
        means: numpy array   - moyennes de chaque feature (de standardize)
        stds: numpy array    - Ã©carts-types de chaque feature (de standardize)
        feature_names: list  - noms des features (de load_data)
        filepath: str        - chemin du fichier JSON Ã  crÃ©er

    Description:
        Sauvegarde tout ce dont logreg_predict.py aura besoin :
        - feature_names : pour savoir quelles colonnes extraire du test set
        - means, stds   : pour normaliser le test set de la mÃªme faÃ§on
        - weights       : pour calculer les probabilitÃ©s et prÃ©dire

    Note:
        JSON ne comprend pas les numpy arrays â†’ .tolist() pour convertir
    """
    # CrÃ©er le dictionnaire avec toutes les donnÃ©es nÃ©cessaires
    data = {
        'feature_names': feature_names,
        'means': means.tolist(),
        'stds': stds.tolist(),
        'weights': {
            house: theta.tolist()
            for house, theta in weights.items()
        }
    }

    # Ã‰crire dans le fichier JSON
    with open(filepath, 'w') as f:
        json.dump(data, f, indent=2)

    print(f"âœ… Poids sauvegardÃ©s dans : {filepath}")
    print(f"   Features : {len(feature_names)}")
    print(f"   Maisons  : {list(weights.keys())}")


def main():
    """Point d'entrÃ©e principal du programme"""
    # RÃ©cupÃ©rer le fichier
    if len(sys.argv) > 1:
        filepath = sys.argv[1]
    else:
        filepath = 'data/dataset_train.csv'

    print("=" * 60)
    print("âš¡ POUDLARD - ENTRAÃNEMENT DU MODÃˆLE âš¡")
    print("=" * 60)
    print()

    # Ã‰tape 1 : Charger les donnÃ©es
    print("ğŸ“‚ Ã‰TAPE 1/4 : Chargement des donnÃ©es")
    print("-" * 60)

    # Option 1 : Utiliser toutes les features (par dÃ©faut)
    X, y, feature_names = load_data(filepath)

    # Option 2 : Utiliser seulement les meilleures features (d'aprÃ¨s pair_plot)
    # D'aprÃ¨s ton analyse, tu pourrais sÃ©lectionner par exemple :
    # selected = ['Herbology', 'Ancient Runes', 'Astronomy',
    #             'Defense Against the Dark Arts']
    # X, y, feature_names = load_data(filepath, selected_features=selected)

    print()

    # Ã‰tape 2 : GÃ©rer les NaN
    print("ğŸ”§ Ã‰TAPE 2/4 : Gestion des valeurs manquantes")
    print("-" * 60)
    X = handle_missing_values(X)
    # VÃ©rification : plus aucun NaN
    remaining_nans = np.sum(np.isnan(X))
    if remaining_nans == 0:
        print("âœ… VÃ©rification : 0 NaN restant dans X")
    else:
        print(f"âš ï¸  Attention : {remaining_nans} NaN encore prÃ©sents!")
    print()

    # Ã‰tape 3 : Standardiser (normalisation z-score)
    print("ğŸ“Š Ã‰TAPE 3/4 : Standardisation (z-score)")
    print("-" * 60)
    X, means, stds = standardize(X)
    print()

    # Ã‰tape 4 : Ajouter la colonne d'intercept
    print("â• Ã‰TAPE 4/4 : Ajout de la colonne d'intercept")
    print("-" * 60)
    X = add_intercept(X)
    print()

    print("=" * 60)
    print("âœ… PRÃ‰PARATION DES DONNÃ‰ES TERMINÃ‰E")
    print("=" * 60)
    print(f"   Shape finale de X : {X.shape}")
    print(f"   Nombre d'Ã©chantillons : {X.shape[0]}")
    print(f"   Nombre de features (+intercept) : {X.shape[1]}")
    print()

    # Ã‰tape 5 : EntraÃ®ner le modÃ¨le
    print("ğŸ§  Ã‰TAPE 5 : EntraÃ®nement (One vs All)")
    print("-" * 60)
    weights = train_one_vs_all(X, y, learning_rate=0.1, epochs=1000)
    print()

    # Ã‰tape 6 : Sauvegarder les poids
    print("ğŸ’¾ Ã‰TAPE 6 : Sauvegarde des poids")
    print("-" * 60)
    save_weights(weights, means, stds, feature_names)
    print()
    print("=" * 60)
    print("âš¡ ENTRAÃNEMENT TERMINÃ‰ - PrÃªt pour la prÃ©diction !")
    print("=" * 60)


if __name__ == '__main__':
    main()
