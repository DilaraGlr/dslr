import sys
import pandas as pd
import numpy as np
import json


def load_data(filepath, selected_features=None):
    """Charge les donn√©es depuis le CSV et extrait X et y"""
    # 1. Charger le CSV
    try:
        df = pd.read_csv(filepath)
        print(f"‚úÖ Fichier charg√© : {filepath}")
        print(f"   {len(df)} √©tudiants trouv√©s")
    except FileNotFoundError:
        print(f"‚ùå Erreur : Le fichier {filepath} n'existe pas")
        sys.exit(1)
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement : {e}")
        sys.exit(1)

    # 2. Extraire y (labels - les maisons)
    if 'Hogwarts House' not in df.columns:
        print("‚ùå Erreur : Colonne 'Hogwarts House' introuvable")
        sys.exit(1)

    y = df['Hogwarts House'].copy()

    # V√©rifier qu'il n'y a pas de NaN dans y
    nan_count = y.isna().sum()
    if nan_count > 0:
        print(f"‚ö†Ô∏è  Attention : {nan_count} valeurs manquantes dans "
              f"'Hogwarts House'")
        # On retire les lignes avec NaN dans y
        valid_indices = y.notna()
        y = y[valid_indices]
        df = df[valid_indices]
        print(f"   ‚Üí {len(y)} √©tudiants conserv√©s apr√®s nettoyage")

    # 3. Extraire X (features - les cours)
    # Colonnes √† exclure (non-num√©riques ou non-pertinentes)
    non_feature_columns = ['Index', 'Hogwarts House', 'First Name',
                           'Last Name', 'Birthday', 'Best Hand']

    # D√©terminer les features √† utiliser
    if selected_features is not None:
        # Utiliser les features s√©lectionn√©es (d'apr√®s l'analyse EDA)
        feature_columns = selected_features
        print("üéØ Utilisation des features s√©lectionn√©es (analyse EDA)")
    else:
        # Utiliser toutes les colonnes num√©riques
        feature_columns = []
        for col in df.columns:
            if col not in non_feature_columns:
                # V√©rifier que c'est bien num√©rique
                if df[col].dtype in ['float64', 'int64']:
                    feature_columns.append(col)
        print("üìä Utilisation de toutes les features num√©riques")

    if len(feature_columns) == 0:
        print("‚ùå Erreur : Aucune feature num√©rique trouv√©e")
        sys.exit(1)

    # V√©rifier que toutes les features demand√©es existent
    for col in feature_columns:
        if col not in df.columns:
            print(f"‚ùå Erreur : Feature '{col}' introuvable dans le dataset")
            sys.exit(1)

    # Convertir X en numpy array pour les calculs matriciels
    X = df[feature_columns].values

    print(f"‚úÖ Features s√©lectionn√©es : {len(feature_columns)} cours")
    for i, feat in enumerate(feature_columns, 1):
        print(f"   {i}. {feat}")
    print(f"\n‚úÖ Labels : {y.nunique()} maisons")
    print(f"   {sorted(y.unique())}")
    print(f"\nüìê Shape de X : {X.shape} (samples, features)")

    return X, y, feature_columns


def handle_missing_values(X):
    """Remplace les valeurs manquantes (NaN) par la moyenne de leur colonne"""
    # Cr√©er une copie pour ne pas modifier l'original
    X_clean = X.copy()

    m, n = X.shape  # m = nombre d'√©tudiants, n = nombre de features
    total_nans = 0

    # Pour chaque colonne (chaque feature)
    for j in range(n):
        # Extraire la colonne j
        column = X[:, j]

        # Calculer la moyenne
        total = 0.0
        count = 0

        # Parcourir toutes les valeurs de la colonne
        for i in range(m):
            val = column[i]
            # V√©rifier si c'est un NaN
            if not np.isnan(val):
                total += val
                count += 1

        # Si toute la colonne est NaN
        if count == 0:
            mean = 0.0
            print(f"‚ö†Ô∏è  Colonne {j} enti√®rement NaN ‚Üí moyenne = 0")
        else:
            mean = total / count

        # Compter et remplacer les NaN dans cette colonne
        # Utiliser un masque bool√©en pour identifier les NaN
        nan_mask = np.isnan(X_clean[:, j])
        nan_count = np.sum(nan_mask)

        if nan_count > 0:
            # Remplacer tous les NaN de cette colonne par la moyenne
            X_clean[nan_mask, j] = mean
            total_nans += nan_count

    print(f"‚úÖ Valeurs manquantes trait√©es : {total_nans} NaN remplac√©s")
    if total_nans > 0:
        print("   Strat√©gie : remplacement par la moyenne de chaque colonne")

    return X_clean


def standardize(X):
    """Mettre toutes les features √† la m√™me √©chelle (z-score normalization)
    Description:
        Pour chaque colonne :
        1. Calculer manuellement la moyenne Œº
        2. Calculer manuellement l'√©cart-type œÉ
           œÉ = ‚àö(Œ£(x - Œº)¬≤ / m)
        3. Normaliser : x_norm = (x - Œº) / œÉ
    """
    m, n = X.shape
    X_norm = X.copy()

    # Tableaux pour stocker les moyennes et √©carts-types
    means = np.zeros(n)
    stds = np.zeros(n)

    # Pour chaque colonne (chaque feature)
    for j in range(n):
        column = X[:, j]

        # 1. Calculer la moyenne
        total = 0.0
        for i in range(m):
            total += column[i]
        mean = total / m
        means[j] = mean

        # 2. Calculer l'√©cart-type
        # std = ‚àö(Œ£(x - mean)¬≤ / m)
        sum_squared_diff = 0.0
        for i in range(m):
            diff = column[i] - mean
            sum_squared_diff += diff * diff

        variance = sum_squared_diff / m
        std = variance ** 0.5  # Racine carr√©e
        stds[j] = std

        # 3. Normaliser la colonne
        # Si std = 0 (colonne constante), on ne divise pas pour √©viter NaN
        if std > 0:
            X_norm[:, j] = (column - mean) / std
        else:
            # Colonne constante ‚Üí on centre juste (- mean)
            X_norm[:, j] = column - mean
            print(f"‚ö†Ô∏è  Colonne {j} a un √©cart-type nul ‚Üí centr√©e seulement")

    print("‚úÖ Standardisation termin√©e (z-score)")
    print(f"   Moyennes : min={means.min():.2f}, max={means.max():.2f}")
    print(f"   √âcarts-types : min={stds.min():.2f}, max={stds.max():.2f}")
    print(f"   X_norm : min={X_norm.min():.2f}, max={X_norm.max():.2f}")

    return X_norm, means, stds


def add_intercept(X):
    """
    Ajoute une colonne de 1 au d√©but de la matrice X (intercept/biais)

    Description:
        Ajoute une colonne de 1 tout √† gauche de X.
        Cette colonne permet de calculer le terme d'intercept (Œ∏‚ÇÄ)
        automatiquement dans le produit matriciel z = X @ Œ∏

        Exemple:
            X = [[x1, x2],     ‚Üí    X_intercept = [[1, x1, x2],
                 [x3, x4]]                          [1, x3, x4]]

        Shape : (m, n) ‚Üí (m, n+1)
    """
    m = X.shape[0]  # Nombre d'√©chantillons

    # Cr√©er une colonne de 1 de taille (m,)
    ones = np.ones(m)

    # Concat√©ner horizontalement : [colonne de 1] + [X]
    # np.c_ permet de concat√©ner des colonnes
    X_with_intercept = np.c_[ones, X]

    print("‚úÖ Colonne d'intercept ajout√©e")
    print(f"   Shape avant : {X.shape}")
    print(f"   Shape apr√®s : {X_with_intercept.shape}")

    return X_with_intercept


def sigmoid(z):
    """
    Transforme un score brut en probabilit√© (entre 0 et 1)"""
    # Limiter z pour √©viter l'overflow num√©rique
    z = np.clip(z, -500, 500)

    # Appliquer la formule sigmoid
    return 1 / (1 + np.exp(-z))


def cost_function(h, y):
    """
    Calcule la log-loss (Binary Cross-Entropy)

    Args:
        h: numpy array (m,) - probabilit√©s pr√©dites par sigmoid (entre 0 et 1)
        y: numpy array (m,) - vraies valeurs binaires (0 ou 1)

    Returns:
        float - valeur de la loss (plus elle est basse, mieux c'est)

    Formule:
        J = -1/m * Œ£(y * log(h) + (1 - y) * log(1 - h))

        - y * log(h)       : p√©nalise si la r√©ponse est 1 et h proche de 0
        - (1-y) * log(1-h) : p√©nalise si la r√©ponse est 0 et h proche de 1
    """
    m = len(y)

    # Clipper h pour √©viter log(0) qui donne -inf et crashe tout
    h = np.clip(h, 1e-15, 1 - 1e-15)

    # Calculer la log-loss
    # Partie 1 : cas o√π la vraie r√©ponse est 1 ‚Üí y * log(h)
    # Partie 2 : cas o√π la vraie r√©ponse est 0 ‚Üí (1 - y) * log(1 - h)
    loss = -1 / m * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))

    return loss


def compute_gradient(X, h, y):
    """
    Calcule le gradient de la loss par rapport aux poids theta

    Args:
        X: numpy array (m, n+1) - donn√©es avec intercept
        h: numpy array (m,)    - probabilit√©s pr√©dites par sigmoid
        y: numpy array (m,)    - vraies valeurs binaires (0 ou 1)

    Returns:
        grad: numpy array (n+1,) - gradient pour chaque poids

    Formule vectoris√©e:
        grad = (1/m) * X.T @ (h - y)

        - (h - y)   : erreur entre pr√©diction et v√©rit√© (m,)
        - X.T       : X transpos√©, shape (n+1, m)
        - X.T @ (h-y) : produit matriciel ‚Üí un gradient par poids (n+1,)
        - 1/m       : moyenne sur tous les √©tudiants
    """
    m = len(y)

    grad = (1 / m) * X.T @ (h - y)

    return grad


def train_one_vs_all(X, y, learning_rate=0.1, epochs=1000, mode='batch'):
    """
    Entra√Æne 4 classifieurs binaires (un par maison)

    Args:
        X: numpy array (m, n+1) - donn√©es avec intercept
        y: pandas Series        - labels (noms des maisons)
        learning_rate: float    - taille du pas du gradient descent
        epochs: int             - nombre d'it√©rations d'entra√Ænement
        mode: str               - 'batch' (tout le dataset) ou 'sgd' (√©l√®ve par √©l√®ve)

    Returns:
        weights: dict - {'Gryffindor': theta, 'Slytherin': theta, ...}
                        un tableau de poids pour chaque maison

    Description:
        Pour chaque maison :
        1. Convertir y en vecteur binaire (1 = cette maison, 0 = autre)
        2. Initialiser theta √† z√©ro
        3. R√©p√©ter `epochs` fois :
           - Mode batch: met √† jour theta sur tout le dataset √† la fois
           - Mode SGD: met √† jour theta √©l√®ve par √©l√®ve dans un ordre al√©atoire
    """
    # R√©cup√©rer les 4 maisons uniques
    houses = sorted(y.unique())

    # Dictionnaire pour stocker les poids de chaque maison
    weights = {}

    print(f"Entra√Ænement : {len(houses)} maisons, "
          f"{epochs} epochs, lr={learning_rate}")
    print()

    # Pour chaque maison
    for house in houses:
        print(f"  üè∞ {house}...")

        # 1. Convertir y en vecteur binaire
        # 1 = c'est cette maison, 0 = c'est une autre maison
        y_binary = (y == house).astype(int).values

        # 2. Initialiser theta √† z√©ro (un poids par feature + biais)
        theta = np.zeros(X.shape[1])

        # 3. Boucle de gradient descent
        for epoch in range(epochs):
            if mode == 'batch':
                # MODE BATCH : mettre √† jour theta sur tout le dataset
                # a. Calculer les probabilit√©s pr√©dites
                h = sigmoid(X @ theta)

                # b. Mesurer l'erreur (loss)
                loss = cost_function(h, y_binary)

                # c. Calculer le gradient
                grad = compute_gradient(X, h, y_binary)

                # d. Ajuster les poids
                theta = theta - learning_rate * grad

            elif mode == 'sgd':
                # MODE SGD : mettre √† jour theta √©l√®ve par √©l√®ve
                # 1. M√©langer les indices al√©atoirement
                m = X.shape[0]  # Nombre d'√©tudiants
                indices = np.random.permutation(m)

                # 2. Boucler √©l√®ve par √©l√®ve
                for i in indices:
                    # Extraire UN seul √©l√®ve (shape (1, n+1) et (1,))
                    Xi = X[i:i+1]
                    yi = y_binary[i:i+1]

                    # Calculer h, grad pour cet √©l√®ve
                    h_i = sigmoid(Xi @ theta)
                    grad_i = compute_gradient(Xi, h_i, yi)

                    # Mettre √† jour theta imm√©diatement
                    theta = theta - learning_rate * grad_i

                # Calculer la loss sur TOUT le dataset pour l'affichage
                h = sigmoid(X @ theta)
                loss = cost_function(h, y_binary)

            elif mode == 'mini-batch':
                # MODE MINI-BATCH : mettre √† jour theta par groupes de 32 √©l√®ves
                # 1. M√©langer les indices al√©atoirement
                m = X.shape[0]  # Nombre d'√©tudiants
                indices = np.random.permutation(m)

                # 2. D√©finir la taille des batches
                batch_size = 32

                # 3. Boucler sur chaque batch
                for start in range(0, m, batch_size):
                    # Calculer la fin du batch (ne pas d√©passer m)
                    end = min(start + batch_size, m)

                    # Extraire les indices du batch
                    batch_indices = indices[start:end]

                    # Extraire les donn√©es du batch (shape (32, n+1) ou moins)
                    Xi = X[batch_indices]
                    yi = y_binary[batch_indices]

                    # Calculer h, grad pour ce batch
                    h_batch = sigmoid(Xi @ theta)
                    grad_batch = compute_gradient(Xi, h_batch, yi)

                    # Mettre √† jour theta
                    theta = theta - learning_rate * grad_batch

                # Calculer la loss sur TOUT le dataset pour l'affichage
                h = sigmoid(X @ theta)
                loss = cost_function(h, y_binary)

            # Afficher la progression toutes les 200 epochs
            if (epoch + 1) % 200 == 0:
                print(f"     epoch {epoch + 1}/{epochs} ‚Üí loss = {loss:.4f}")

        # Sauvegarder les poids de cette maison
        weights[house] = theta

    print()
    print(f"‚úÖ Entra√Ænement termin√© ! {len(weights)} mod√®les entra√Æn√©s")

    return weights


def save_weights(weights, means, stds, feature_names, filepath='weights.json'):
    """
    Sauvegarde les poids et param√®tres de normalisation dans un fichier JSON

    Args:
        weights: dict        - {'Gryffindor': theta, ...} (numpy arrays)
        means: numpy array   - moyennes de chaque feature (de standardize)
        stds: numpy array    - √©carts-types de chaque feature (de standardize)
        feature_names: list  - noms des features (de load_data)
        filepath: str        - chemin du fichier JSON √† cr√©er

    Description:
        Sauvegarde tout ce dont logreg_predict.py aura besoin :
        - feature_names : pour savoir quelles colonnes extraire du test set
        - means, stds   : pour normaliser le test set de la m√™me fa√ßon
        - weights       : pour calculer les probabilit√©s et pr√©dire

    Note:
        JSON ne comprend pas les numpy arrays ‚Üí .tolist() pour convertir
    """
    # Cr√©er le dictionnaire avec toutes les donn√©es n√©cessaires
    data = {
        'feature_names': feature_names,
        'means': means.tolist(),
        'stds': stds.tolist(),
        'weights': {
            house: theta.tolist()
            for house, theta in weights.items()
        }
    }

    # √âcrire dans le fichier JSON
    with open(filepath, 'w') as f:
        json.dump(data, f, indent=2)

    print(f"‚úÖ Poids sauvegard√©s dans : {filepath}")
    print(f"   Features : {len(feature_names)}")
    print(f"   Maisons  : {list(weights.keys())}")


def main():
    """Point d'entr√©e principal du programme"""
    # R√©cup√©rer le fichier et le mode d'entra√Ænement
    filepath = 'data/dataset_train.csv'
    mode = 'batch'  # Mode par d√©faut

    # Parcourir les arguments
    for arg in sys.argv[1:]:
        if arg == '--sgd':
            mode = 'sgd'
        elif arg == '--mini-batch':
            mode = 'mini-batch'
        elif not arg.startswith('-'):
            # C'est le fichier de donn√©es
            filepath = arg

    print("=" * 60)
    print("‚ö° POUDLARD - ENTRA√éNEMENT DU MOD√àLE ‚ö°")
    print("=" * 60)
    print(f"   Mode d'entra√Ænement : {mode.upper()}")
    if mode == 'batch':
        print("   (Batch Gradient Descent - tout le dataset √† la fois)")
    elif mode == 'sgd':
        print("   (Stochastic Gradient Descent - √©l√®ve par √©l√®ve)")
    elif mode == 'mini-batch':
        print("   (Mini-Batch Gradient Descent - groupes de 32 √©l√®ves)")
    print()

    # √âtape 1 : Charger les donn√©es
    print("üìÇ √âTAPE 1/4 : Chargement des donn√©es")
    print("-" * 60)

    # Option 1 : Utiliser toutes les features (par d√©faut)
    X, y, feature_names = load_data(filepath)

    # Option 2 : Utiliser seulement les meilleures features (d'apr√®s pair_plot)
    # D'apr√®s ton analyse, tu pourrais s√©lectionner par exemple :
    # selected = ['Herbology', 'Ancient Runes', 'Astronomy',
    #             'Defense Against the Dark Arts']
    # X, y, feature_names = load_data(filepath, selected_features=selected)

    print()

    # √âtape 2 : G√©rer les NaN
    print("üîß √âTAPE 2/4 : Gestion des valeurs manquantes")
    print("-" * 60)
    X = handle_missing_values(X)
    # V√©rification : plus aucun NaN
    remaining_nans = np.sum(np.isnan(X))
    if remaining_nans == 0:
        print("‚úÖ V√©rification : 0 NaN restant dans X")
    else:
        print(f"‚ö†Ô∏è  Attention : {remaining_nans} NaN encore pr√©sents!")
    print()

    # √âtape 3 : Standardiser (normalisation z-score)
    print("üìä √âTAPE 3/4 : Standardisation (z-score)")
    print("-" * 60)
    X, means, stds = standardize(X)
    print()

    # √âtape 4 : Ajouter la colonne d'intercept
    print("‚ûï √âTAPE 4/4 : Ajout de la colonne d'intercept")
    print("-" * 60)
    X = add_intercept(X)
    print()

    print("=" * 60)
    print("‚úÖ PR√âPARATION DES DONN√âES TERMIN√âE")
    print("=" * 60)
    print(f"   Shape finale de X : {X.shape}")
    print(f"   Nombre d'√©chantillons : {X.shape[0]}")
    print(f"   Nombre de features (+intercept) : {X.shape[1]}")
    print()

    # √âtape 5 : Entra√Æner le mod√®le
    print("üß† √âTAPE 5 : Entra√Ænement (One vs All)")
    print("-" * 60)
    weights = train_one_vs_all(X, y, learning_rate=0.1, epochs=1000, mode=mode)
    print()

    # √âtape 6 : Sauvegarder les poids
    print("üíæ √âTAPE 6 : Sauvegarde des poids")
    print("-" * 60)
    save_weights(weights, means, stds, feature_names)
    print()
    print("=" * 60)
    print("‚ö° ENTRA√éNEMENT TERMIN√â - Pr√™t pour la pr√©diction !")
    print("=" * 60)


if __name__ == '__main__':
    main()
